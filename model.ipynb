{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmin27/Anticipate-Accident/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBSUfCOnAcxm"
      },
      "source": [
        "# Setting the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIBPZNucIk-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad999d8f-4d3c-44da-b598-0a27da067324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t605pNz-l5Em",
        "outputId": "1b5c8717-76e0-46cb-9c62-a9b35ff62070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import resize\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow  # for colab env.\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Dict, Iterable, Callable\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "EPOCH = 3\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset\n"
      ],
      "metadata": {
        "id": "Qa43vC07Nter"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) OpenCV\n",
        "- use OpenCV\n",
        "- make the several captures for a video\n",
        "- make the video with cv2.VideoWriter"
      ],
      "metadata": {
        "id": "bC0rjzmgsYDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB-tl8vnIQud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac1b1e73-3a8e-435e-8719-e212a8d46af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 갯수: 100\n",
            "1280 720\n"
          ]
        }
      ],
      "source": [
        "# training-positive-000001\n",
        "PATH = '000001.mp4'\n",
        "\n",
        "cap = cv2.VideoCapture(PATH)\n",
        "frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print('Frame 갯수:', frame_cnt)\n",
        "\n",
        "if cap.isOpened() == False:\n",
        "  print(\"Can't open vieo...\")\n",
        "\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "print(frame_width, frame_height)\n",
        "\n",
        "save_name = f\"{PATH.split('.')[0]}_result\"\n",
        "out = cv2.VideoWriter(f\"{save_name}.mp4\",\n",
        "                      cv2.VideoWriter_fourcc(*'DIVX'), 20,\n",
        "                      (frame_width, frame_height))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(2) Torchvision\n",
        "- read_video"
      ],
      "metadata": {
        "id": "o5bX4Va4CBHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av"
      ],
      "metadata": {
        "id": "1F1aqwnnOcz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac977b8b-921a-47a5-f30b-f9f397579562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-12.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DashcamDataset(Dataset):\n",
        "  def __init__(self, data_path, train=True, transform=None):\n",
        "    self.data_path = data_path\n",
        "    self.transform = transform\n",
        "\n",
        "    if train == True:\n",
        "      self.base_path = os.path.join(data_path, \"training\")\n",
        "    else:\n",
        "      self.base_path = os.path.join(data_path, \"testing\")\n",
        "\n",
        "    self.positive_path = os.path.join(self.base_path, \"positive\")\n",
        "    self.negative_path = os.path.join(self.base_path, \"negative\")\n",
        "\n",
        "    self.positive_videos = [os.path.join(self.positive_path, v) for v in sorted(os.listdir(self.positive_path))]\n",
        "    self.negative_videos = [os.path.join(self.negative_path, v) for v in sorted(os.listdir(self.negative_path))]\n",
        "\n",
        "    self.video_paths = self.positive_videos + self.negative_videos\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.video_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    video_path = self.video_paths[idx]\n",
        "    video = torchvision.io.read_video(video_path, output_format = 'TCHW')[0]\n",
        "    video = torch.stack([resize(frame, (180, 320)) for frame in video])  # Resize\n",
        "    label = 1 if video_path in self.positive_videos else 0\n",
        "\n",
        "    return video, label\n"
      ],
      "metadata": {
        "id": "WJRI3XbsCl5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Random Sampling\n",
        "# path = \"/content/drive/MyDrive/Study/2024-S/Car_Accident_Anticipation/프로젝트/Dashcam_dataset/videos\"\n",
        "path = \"/content/drive/MyDrive/Study/Car_Accident_Anticipation/프로젝트/Dashcam_dataset/videos\"\n",
        "\n",
        "train_sampled_dataset = DashcamDataset(path, train=True)\n",
        "\n",
        "indices = list(range(642))\n",
        "\n",
        "sampler = SubsetRandomSampler(indices)\n",
        "train_sampled_loader = DataLoader(train_sampled_dataset, batch_size=BATCH_SIZE, sampler=sampler)"
      ],
      "metadata": {
        "id": "6di721s_9KkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = \"/content/drive/MyDrive/Study/2024-S/Car_Accident_Anticipation/프로젝트/Dashcam_dataset/videos\"\n",
        "path = \"/content/drive/MyDrive/Study/Car_Accident_Anticipation/프로젝트/Dashcam_dataset/videos\"\n",
        "\n",
        "train_dataset = DashcamDataset(path, train=True)\n",
        "test_dataset = DashcamDataset(path, train=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "-YCqRhYWDeof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))  # 1284\n",
        "print(len(test_dataset))  # 466\n",
        "print(len(train_loader))  # 321\n",
        "print(len(test_loader))  # 117\n",
        "print(len(train_sampled_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7YKOat6sy4z",
        "outputId": "275fe4fc-6786-4e74-92ff-996f5ed028b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1284\n",
            "466\n",
            "321\n",
            "117\n",
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video, label = next(iter(train_sampled_loader))"
      ],
      "metadata": {
        "id": "0NJigibuLn9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(video))\n",
        "print(video.shape)\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FartrAqUPCXA",
        "outputId": "13aca6b9-91ba-4d82-9762-38c490822f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "torch.Size([16, 100, 3, 180, 320])\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "frame = video[0][0]\n",
        "\n",
        "plt.imshow(frame.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AOO_UGJ3soqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz1MGzfK3-_Q"
      },
      "source": [
        "# Object Detection\n",
        "- faster R CNN pretrained model with ResNet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7QJa6Q-rBTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf7062d-62e3-4b81-f708-652a399d1bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 157MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "fasterrcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights,\n",
        "                                                                  rpn_post_nms_top_n_train=20,\n",
        "                                                                  rpn_post_nms_top_n_test=20,\n",
        "                                                                  box_detections_per_img=20)\n",
        "fasterrcnn = fasterrcnn.eval().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference with Faster R CNN\n"
      ],
      "metadata": {
        "id": "iSyincDz_mw1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmAcrFIHspHb",
        "outputId": "e34b7985-4514-4b8e-b14b-6a7365fa123f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
            "The number of labels:  91\n"
          ]
        }
      ],
      "source": [
        "class_labels = weights.meta[\"categories\"]\n",
        "print(class_labels)\n",
        "print(\"The number of labels: \", len(class_labels))  # 우리가 사용한 라벨들만 좀 추리기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpy8n9RVnWUg"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "def predict(image, model, detection_threshold):\n",
        "  image = transform(image).to(device) # to tensor\n",
        "  image = image.unsqueeze(0)\n",
        "  outputs = fasterrcnn(image)\n",
        "  # Input: batch_size(1) * c * h * w, (1, 3, 720, 1280)\n",
        "  # Output: [{'scores': [score1, score2, ...], 'boxes': [box1, box2, ...], 'labels': [label1, label2, ...]}]\n",
        "\n",
        "  pred_classes = [class_labels[i] for i in outputs[0]['labels'].cpu().numpy()]  # class_labels는 따로 정의하기(car, motorbike 등)\n",
        "  pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
        "  pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
        "  pred_labels = outputs[0]['labels'].cpu().numpy()\n",
        "\n",
        "  boxes = pred_bboxes[pred_scores >= detection_threshold]\n",
        "\n",
        "  print(\"# of candidate objects: \", len(boxes))\n",
        "\n",
        "  return boxes, pred_classes, outputs[0]['labels']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIfkw-SJ1E3i"
      },
      "outputs": [],
      "source": [
        "COLORS = np.random.uniform(0, 255, size=(len(class_labels), 3))\n",
        "\n",
        "def draw_boxes(boxes, classes, labels, image):\n",
        "  image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)  #OepnCV에서 BRG로 저장됨(변환 필요)\n",
        "  for i, box in enumerate(boxes):\n",
        "    color = COLORS[labels[i]]\n",
        "    cv2.rectangle(\n",
        "        image,\n",
        "        (int(box[0]), int(box[1])),  # x1, x2\n",
        "        (int(box[2]), int(box[3])),  # y1, y2\n",
        "        color,\n",
        "        2  # thickness\n",
        "    )\n",
        "\n",
        "    cv2.putText(image, classes[i], (int(box[0]), int(box[1]-5)),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2,\n",
        "                lineType=cv2.LINE_AA)\n",
        "\n",
        "  return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB8fDpnU4HkU"
      },
      "outputs": [],
      "source": [
        "total_frame = 0\n",
        "frame_list = []\n",
        "\n",
        "while cap.isOpened():\n",
        "  ret, img_frame = cap.read()\n",
        "\n",
        "  if not ret:\n",
        "    print('남은 프레임이 없습니다.')\n",
        "    break\n",
        "\n",
        "  total_frame += 1\n",
        "  print(\"Frame: \", total_frame)\n",
        "\n",
        "  frame_list.append(img_frame)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    boxes, classes, labels = predict(img_frame, fasterrcnn, 0.8)\n",
        "\n",
        "  image = draw_boxes(boxes, classes, labels, img_frame)\n",
        "\n",
        "  # cv2_imshow(image)\n",
        "  # out.write(image)\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(total_frame)\n",
        "print(frame_list[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = frames.float().to(device)  # (100, 3, 720, 1280)\n",
        "\n",
        "for i in range(input.size(0)):\n",
        "  frame = input[i, :, :, :]\n",
        "  print(frame.shape)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    boxes, classes, labels = predict(frame, fasterrcnn, 0.7)\n",
        "\n",
        "# cv2_imshow(image)"
      ],
      "metadata": {
        "id": "cJ24hNSRPcsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction\n"
      ],
      "metadata": {
        "id": "za5tIl9pX0HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, model: nn.Module, layers: Iterable[str]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.layers = layers\n",
        "        self._features = {layer: torch.empty(0) for layer in layers}\n",
        "\n",
        "        for layer_id in layers:\n",
        "            layer = dict([*self.model.named_modules()])[layer_id]\n",
        "            layer.register_forward_hook(self.save_outputs_hook(layer_id))\n",
        "\n",
        "    def save_outputs_hook(self, layer_id: str) -> Callable:\n",
        "        def fn(_, __, output):\n",
        "            self._features[layer_id] = output\n",
        "        return fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.model(x)\n",
        "        return self._features, result"
      ],
      "metadata": {
        "id": "lqaJLGMoMyO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try"
      ],
      "metadata": {
        "id": "Ja_54c_5K1B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 예시 테스트\n",
        "\n",
        "features_extractor = FeatureExtractor(fasterrcnn, [\"roi_heads.box_head.fc7\"]).eval()\n",
        "frames = frames.float().to(device)\n",
        "\n",
        "roi_features = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(frames.size(0)):\n",
        "    frame = frames[i, :, :, :].unsqueeze(0)\n",
        "    # print(frame)\n",
        "\n",
        "    feature, result = features_extractor(frame)\n",
        "    roi_features.append(feature[\"roi_heads.box_head.fc7\"])\n",
        "\n",
        "roi_features = torch.stack(roi_features, dim=0)"
      ],
      "metadata": {
        "id": "JsLvXBa89g7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE4PDUUSOJF4"
      },
      "source": [
        "# Full-frame feature\n",
        "- pre-trained VGG model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLDdazvPOIov"
      },
      "outputs": [],
      "source": [
        "vgg = torchvision.models.vgg16(pretrained=True).to(device)\n",
        "print(vgg)\n",
        "\n",
        "vgg.classifier = vgg.classifier[:-1]  # 마지막 레이어 삭제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpq_gIDKO6Qn",
        "outputId": "cfba6b85-1e4f-4788-ec30-53a7e9081005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.4001]],\n",
            "       device='cuda:0')\n",
            "torch.Size([1, 4096])\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.CenterCrop((224, 224)),  # 원하는 크기로 조절\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "sample_img = frame_list[0]\n",
        "sample_img = transform(sample_img).unsqueeze(0).to(device)\n",
        "print(sample_img.shape)\n",
        "\n",
        "with torch.no_grad():\n",
        "  feature = vgg(sample_img)\n",
        "\n",
        "print(feature)\n",
        "print(feature.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "lrSBqMdT55nB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXiiKZ_65tEO"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, encoder_dim):\n",
        "    super(Attention, self).__init__()\n",
        "    self.U = nn.Linear(1024, 512)\n",
        "    self.W = nn.Linear(encoder_dim, 512)\n",
        "    self.w = nn.Linear(512, 1)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.softmax = nn.Softmax(1)\n",
        "\n",
        "  def forward(self, img_features, hidden_state):\n",
        "    W_e = self.W(img_features)  # (batch, 100, 20, 512)\n",
        "    U_e = self.U(hidden_state).unsqueeze(0).permute(2, 0, 1, 3)  # (batch, 1, 1, 512)\n",
        "    att = self.tanh(W_e + U_e)  # (batch, 100, 20, 512)\n",
        "    e = self.w(att).squeeze(3)  # (batch, 100, 20)\n",
        "    alpha = self.softmax(e)  # (batch, 100, 20)\n",
        "    phi = (img_features * alpha.unsqueeze(3)).sum(2)  # phi(x, alpha), (batch, 100, 1024)\n",
        "    return phi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try"
      ],
      "metadata": {
        "id": "hG11dE3aK_HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 예시 테스트\n",
        "\n",
        "features = torch.randn(4, 100, 20, 1024).to(device)\n",
        "hidden = torch.randn(1, 4, 1024).to(device)\n",
        "\n",
        "attention = Attention(encoder_dim = 1024).to(device)\n",
        "phi= attention(features, hidden)\n",
        "\n",
        "print(phi.shape)\n",
        "# print(phi)"
      ],
      "metadata": {
        "id": "dsfjSCn_dyTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80131996-2f6d-431c-b270-df5ebbaf58a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 100, 20, 512])\n",
            "torch.Size([4, 1, 1, 512])\n",
            "torch.Size([4, 100, 20, 512])\n",
            "torch.Size([4, 100, 20])\n",
            "torch.Size([4, 100, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder\n"
      ],
      "metadata": {
        "id": "EX9danE859Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch size videos\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.model = fasterrcnn\n",
        "    self.layer = \"roi_heads.box_head.fc7\"\n",
        "    self.features_extractor = FeatureExtractor(self.model, [self.layer]).eval()\n",
        "\n",
        "  def forward(self, videos):\n",
        "    batch_size = frames.size(0)\n",
        "    frames_size = frames.size(1)\n",
        "    video_features = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      video = videos[i]\n",
        "      # print(f\"{i+1}th video: {video.shape}\")\n",
        "\n",
        "      roi_features = []\n",
        "\n",
        "      for j in range(frames_size):\n",
        "        with torch.no_grad():\n",
        "          frame = video[j, :, :, :].unsqueeze(0)  # (1, 3, 640, 1280)\n",
        "          feature, _ = self.features_extractor(frame)\n",
        "\n",
        "          roi_features.append(feature[self.layer])\n",
        "\n",
        "      roi_features = torch.stack(roi_features, dim=0)  # 모든 frames 하나의 tensor로, [100, 20, 1024]\n",
        "      video_features.append(roi_features)\n",
        "\n",
        "    video_features = torch.stack(video_features, dim=0)  # 모든 video 하나의 tensor로, [batch, 100, 20, 1024]\n",
        "    # print(f\"Video Features: {video_features.shape}\")\n",
        "\n",
        "    return video_features"
      ],
      "metadata": {
        "id": "C82DzZZnr8gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try"
      ],
      "metadata": {
        "id": "SJN1Wj4cSjvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for one video\n",
        "\n",
        "class Encoder_one(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Encoder_one, self).__init__()\n",
        "    self.model = fasterrcnn\n",
        "    self.layer = \"roi_heads.box_head.fc7\"\n",
        "    self.features_extractor = FeatureExtractor(self.model, [self.layer]).eval()\n",
        "\n",
        "  def forward(self, frames):\n",
        "    roi_features = []\n",
        "\n",
        "    for i in range(frames.size(0)):\n",
        "      with torch.no_grad():\n",
        "        frame = frames[i, :, :, :].unsqueeze(0)  # (1, 3, 640, 1280)\n",
        "        feature, _ = self.features_extractor(frame)\n",
        "\n",
        "        roi_features.append(feature[self.layer])\n",
        "\n",
        "    roi_features = torch.stack(roi_features, dim=0)\n",
        "\n",
        "    return roi_features"
      ],
      "metadata": {
        "id": "CcWka1-du-ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-m161X4ch37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a4cc9fa-c666-48e6-d0db-ab1416c6a672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1th video: torch.Size([100, 3, 360, 640])\n",
            "2th video: torch.Size([100, 3, 360, 640])\n",
            "3th video: torch.Size([100, 3, 360, 640])\n",
            "4th video: torch.Size([100, 3, 360, 640])\n",
            "total video: torch.Size([4, 100, 20, 1024])\n",
            "torch.Size([4, 100, 20, 1024])\n"
          ]
        }
      ],
      "source": [
        "frames = torch.randn(4, 100, 3, 360, 640).to(device)\n",
        "\n",
        "encoder = Encoder()\n",
        "features = encoder(frames)\n",
        "\n",
        "print(features.shape)  # (batch_size, 100, 20, 1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder\n",
        "- LSTM"
      ],
      "metadata": {
        "id": "WRR0NIJOXLNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DSA_LSTM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DSA_LSTM, self).__init__()\n",
        "\n",
        "    self.dsa = Attention(encoder_dim = 1024).to(device)\n",
        "    self.lstm = nn.LSTM(input_size = 1024,\n",
        "                        hidden_size = 1024,\n",
        "                        batch_first = True).to(device)\n",
        "                        # batch_first = [batch, time_step, input]\n",
        "\n",
        "    self.prediction = nn.Linear(1024, 1).to(device)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, features):\n",
        "    h0 = torch.randn(1, features.size(0), 1024).to(device)\n",
        "    c0 = torch.randn(1, features.size(0), 1024).to(device)\n",
        "\n",
        "    input = self.dsa(features, h0)\n",
        "    out, _ = self.lstm(input, (h0, c0))\n",
        "    out = self.prediction(out)\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "BkHv2jBwXK_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try\n"
      ],
      "metadata": {
        "id": "McIT9u9gSmA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = torch.randn(4, 100, 20, 1024).to(device)\n",
        "\n",
        "lstm = DSA_LSTM()\n",
        "result = lstm(features)\n",
        "\n",
        "print(result)\n",
        "print(result.shape)\n"
      ],
      "metadata": {
        "id": "-LMQDFv0ucvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "NwJyhDlCpUqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDubo1qvKE0L"
      },
      "outputs": [],
      "source": [
        "# a_0 : Probability of Accident\n",
        "# a_1 : Probability of Non-Accident\n",
        "# p(prob, frame)\n",
        "# t(prob, frame)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def prediction(pred):  # [100]\n",
        "  pred = torch.stack([torch.tensor([pred, frame]) for frame, pred in enumerate(pred)])\n",
        "  return pred\n",
        "\n",
        "\n",
        "def CrossEntropyLoss(pred):  # for negative\n",
        "  label = torch.stack([p[0] for p in pred])\n",
        "  loss = -torch.sum(torch.log(1-label))\n",
        "  loss.requires_grad = True\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "def AnticipationLoss(pred):  # for positive\n",
        "  label = torch.stack([p[0] for p in pred])  # pred prob\n",
        "  timestep = torch.stack([p[1] for p in pred]).to(int)\n",
        "  loss = -torch.sum(torch.exp(-torch.maximum(torch.tensor(0), (90 - timestep)))*torch.log(label))\n",
        "  loss.requires_grad = True\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder().eval()\n",
        "lstm = DSA_LSTM().to(device).train()\n",
        "\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.005)\n",
        "\n",
        "# lr = optimizer.param_groups[0]['lr']\n",
        "# print(\"Learning rate:\", lr)   #default = 0.001"
      ],
      "metadata": {
        "id": "SgJBrrIVjwKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "YtcWkRtHJ9Si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_save = []\n",
        "\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  print(f\"Epoch: {epoch+1}\")\n",
        "\n",
        "  train_loss = 0\n",
        "  step = 0\n",
        "\n",
        "  pbar = tqdm(train_sampled_loader, total=len(train_sampled_loader))\n",
        "  for videos, labels in pbar:\n",
        "    step += 1\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # prediction\n",
        "    frames = videos.float().to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    features = encoder(frames)\n",
        "    preds = lstm(features).squeeze(-1)  # [batch_size, frames=100]\n",
        "\n",
        "    # Loss\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(preds.size(0)):\n",
        "      pred = prediction(preds[i])\n",
        "\n",
        "      if labels[i] == 1:\n",
        "        pos_loss = AnticipationLoss(pred)\n",
        "        loss += pos_loss\n",
        "\n",
        "      else:\n",
        "        neg_loss = CrossEntropyLoss(pred)\n",
        "        loss += neg_loss\n",
        "\n",
        "    # Train loss\n",
        "    total_loss = loss / pred.size(0)\n",
        "    train_loss += total_loss.item()  # Tensor to Scalar\n",
        "\n",
        "    # back propagation\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pbar.set_description(f\"Epoch {epoch+1}/{EPOCH}, Train Loss: {total_loss.item():.4f}\", refresh=True)\n",
        "\n",
        "    if step % 5 == 0:\n",
        "      print(f\"Step {step}/{len(train_sampled_loader)}, Train Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "  avg_train_loss = train_loss / step\n",
        "\n",
        "  train_loss_save.append(avg_train_loss)\n",
        "  print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "  torch.save(lstm.state_dict(), f'DSA_LSTM_train_epoch{epoch+1}.pth')\n"
      ],
      "metadata": {
        "id": "JRYIFGNvtSpj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448,
          "referenced_widgets": [
            "247180979f8b4fdd8358f0e0f109e935",
            "958d6cad7a2f4afdbf5bec3690cf6b94",
            "337984f2db3f4bc5b64d8b3662987b08",
            "035d443fb7af4921bfb3b08f25e97bd8",
            "15d8c0881bdc44b2bb301ff82a808d63",
            "74a607afa4194e8b92186fba04fca7e8",
            "58e602df2c4e47e69d9d998f97d41413",
            "83d6db7255b54632a3b8d9b8f6f30263",
            "68506af777314d07963515de3ad335ba",
            "27565ec442684df3b7f1a318fe67a863",
            "11709677fbfa4324821d60f78be7a673",
            "308f538c6f9d4027a00c0632dcb25c34",
            "2d0857446b984df8a7f3db7493d79799",
            "53d3dd9d2a134b3cb652c1eafcf750ba",
            "1282ec145ce54f10abee9eb7c379ed05",
            "31913361f512435abc719697160222d1",
            "ca965e71cdc7457f93ebbaa089cc8a08",
            "ad7c7ba962ae43079328507555a064dc",
            "7cd8ad1bfd2e4a66bb968dec03478f74",
            "9f71832b8af94cd08a05df47af81b7a7",
            "48934e78d6944c18b4cce1ec3bc08abb",
            "54c5261649014b59bce84345f3ce6698"
          ]
        },
        "outputId": "d0ccd38e-ea55-45ae-8594-1eb00f02bb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "247180979f8b4fdd8358f0e0f109e935"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
            "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5/41, Train Loss: 4.2594\n",
            "Step 10/41, Train Loss: 4.2716\n",
            "Step 15/41, Train Loss: 4.4426\n",
            "Step 20/41, Train Loss: 2.4063\n",
            "Step 25/41, Train Loss: 3.6741\n",
            "Step 30/41, Train Loss: 3.2847\n",
            "Step 35/41, Train Loss: 3.7180\n",
            "Step 40/41, Train Loss: 5.5769\n",
            "Average Train Loss: 4.0824\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "308f538c6f9d4027a00c0632dcb25c34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5/41, Train Loss: 4.3769\n",
            "Step 10/41, Train Loss: 3.1621\n",
            "Step 15/41, Train Loss: 5.6410\n",
            "Step 20/41, Train Loss: 3.9277\n",
            "Step 25/41, Train Loss: 6.3997\n",
            "Step 30/41, Train Loss: 4.4853\n",
            "Step 35/41, Train Loss: 3.8139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(lstm.state_dict(), 'DSA_LSTM_1.pth')\n",
        "\n",
        "model_checkpoint_path = \"/content/drive/MyDrive/Study/2024-S/Car_Accident_Anticipation/DSA_LSTM.pth\"\n",
        "torch.save(lstm.state_dict(), model_checkpoint_path)"
      ],
      "metadata": {
        "id": "pgCeaWvYNESi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the loss values\n",
        "plt.plot(train_loss_save)\n",
        "\n",
        "# Label the axes\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "VCAxmb_iEAiN",
        "outputId": "bc8d21ad-cc97-4e61-90a9-89f5311d3a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loss_save' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-fa7ee0b0f8e9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot the loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Label the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loss_save' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43c8PUZRELxN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "bC0rjzmgsYDQ",
        "iSyincDz_mw1",
        "Ja_54c_5K1B_",
        "aE4PDUUSOJF4",
        "hG11dE3aK_HF",
        "SJN1Wj4cSjvw",
        "McIT9u9gSmA1"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "247180979f8b4fdd8358f0e0f109e935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_958d6cad7a2f4afdbf5bec3690cf6b94",
              "IPY_MODEL_337984f2db3f4bc5b64d8b3662987b08",
              "IPY_MODEL_035d443fb7af4921bfb3b08f25e97bd8"
            ],
            "layout": "IPY_MODEL_15d8c0881bdc44b2bb301ff82a808d63"
          }
        },
        "958d6cad7a2f4afdbf5bec3690cf6b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74a607afa4194e8b92186fba04fca7e8",
            "placeholder": "​",
            "style": "IPY_MODEL_58e602df2c4e47e69d9d998f97d41413",
            "value": "Epoch 1/3, Train Loss: 0.8122: 100%"
          }
        },
        "337984f2db3f4bc5b64d8b3662987b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83d6db7255b54632a3b8d9b8f6f30263",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68506af777314d07963515de3ad335ba",
            "value": 41
          }
        },
        "035d443fb7af4921bfb3b08f25e97bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27565ec442684df3b7f1a318fe67a863",
            "placeholder": "​",
            "style": "IPY_MODEL_11709677fbfa4324821d60f78be7a673",
            "value": " 41/41 [2:26:40&lt;00:00, 161.02s/it]"
          }
        },
        "15d8c0881bdc44b2bb301ff82a808d63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74a607afa4194e8b92186fba04fca7e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e602df2c4e47e69d9d998f97d41413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83d6db7255b54632a3b8d9b8f6f30263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68506af777314d07963515de3ad335ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27565ec442684df3b7f1a318fe67a863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11709677fbfa4324821d60f78be7a673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "308f538c6f9d4027a00c0632dcb25c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d0857446b984df8a7f3db7493d79799",
              "IPY_MODEL_53d3dd9d2a134b3cb652c1eafcf750ba",
              "IPY_MODEL_1282ec145ce54f10abee9eb7c379ed05"
            ],
            "layout": "IPY_MODEL_31913361f512435abc719697160222d1"
          }
        },
        "2d0857446b984df8a7f3db7493d79799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca965e71cdc7457f93ebbaa089cc8a08",
            "placeholder": "​",
            "style": "IPY_MODEL_ad7c7ba962ae43079328507555a064dc",
            "value": "Epoch 2/3, Train Loss: 3.8139:  85%"
          }
        },
        "53d3dd9d2a134b3cb652c1eafcf750ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cd8ad1bfd2e4a66bb968dec03478f74",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f71832b8af94cd08a05df47af81b7a7",
            "value": 35
          }
        },
        "1282ec145ce54f10abee9eb7c379ed05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48934e78d6944c18b4cce1ec3bc08abb",
            "placeholder": "​",
            "style": "IPY_MODEL_54c5261649014b59bce84345f3ce6698",
            "value": " 35/41 [1:59:40&lt;20:31, 205.31s/it]"
          }
        },
        "31913361f512435abc719697160222d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca965e71cdc7457f93ebbaa089cc8a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad7c7ba962ae43079328507555a064dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cd8ad1bfd2e4a66bb968dec03478f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f71832b8af94cd08a05df47af81b7a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48934e78d6944c18b4cce1ec3bc08abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c5261649014b59bce84345f3ce6698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}